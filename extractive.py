# -*- coding: utf-8 -*-
"""extractive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-61Ho6k3oGLxyjGy7IKEEjL0KJg3JTS
"""

import nltk
import numpy as np
import numpy.linalg as LA
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from gensim.models import Word2Vec
import math
import pandas as pd
import tfidf
import gensim.downloader as api
import re
model = api.load('glove-wiki-gigaword-100')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

#If there isn't wikipedia, uncomment the line below
!pip install wikipedia
import wikipedia

stopWords = stopwords.words('english')
lemmatizer = WordNetLemmatizer()

tokenizer=lambda text: [lemmatizer.lemmatize(t.lower()) for t in nltk.word_tokenize(text) if (t not in stopWords)]
vectorizer = CountVectorizer(tokenizer = tokenizer,lowercase = True)
transformer = TfidfTransformer()

def get_related_doc():
  tfidf.initFromFile("tfidf.txt")
  sortedTitles = tfidf.tfidf("United States", nTitles=4)

  documents = list()
  for title in sortedTitles:
    wikisearch = wikipedia.page(title[0]) 
    wikicontent = wikisearch.content
    wikicontent = re.sub(r'[=]+[^=]*[=]+', '', wikicontent)
    documents.append(sent_tokenize(wikicontent))
  return sortedTitles, documents

def get_cosine(text):
  trainVectorizerArray = vectorizer.fit_transform(text).toarray()
  cx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 5)

  doc_length = len(trainVectorizerArray)

  cosine_score = []
  for i in range(doc_length):
      sentence_score = 0
      for j in range(doc_length):
          sentence_score += cx(trainVectorizerArray[i], trainVectorizerArray[j])
      cosine_score.append(math.log10(sentence_score))

  sorted_score_idx = sorted(range(len(cosine_score)), key=lambda k: cosine_score[k], reverse=True)

  candidates = []
  count = 0
  iterate = 0
  while(iterate < len(text) and count < 10):
      if(len(text[sorted_score_idx[iterate]]) < 600 and len(text[sorted_score_idx[iterate]]) > 100):
          candidates.append(text[sorted_score_idx[iterate]])
          count += 1
      iterate += 1

  tokenized_cand = []
  for txt in candidates:
    tokenized_cand.append(tokenizer(txt))

  return tokenized_cand, candidates, doc_length

def get_wmd(title,tokenized_cand, doc_length):
  candidate_length = len(tokenized_cand)
  wmd_score = []
  for i in range(candidate_length):
      distance = 0 
      for j in range(doc_length):
          distance += model.wmdistance(tokenized_cand[i], title)
      wmd_score.append((1/distance))
  sorted_score_idx = sorted(range(len(wmd_score)), key=lambda k: wmd_score[k], reverse=True)
  return sorted_score_idx

def summarize(title, candidates, wmd_score, main):
  string = "Title: " + str(title) + "\n" 
  len = 4
  if main:
    len = 8
  for i in range(len):
    string += candidates[wmd_score[i]] + " "
  return string + "\n\n"

top_titles, documents = get_related_doc()
generated_text = ""
main = True
for title, document in zip(top_titles, documents):
  print(title[0])
  tokenized_cand, candidates, doc_length = get_cosine(document)
  wmd_score = get_wmd(title[0], tokenized_cand, doc_length)
  generated_text += summarize(title[0], candidates, wmd_score, main)
  if main:
    main = False

text_file = open("Related Summarizations.txt", "w")
text_file.write(generated_text)
text_file.close()

print(generated_text)